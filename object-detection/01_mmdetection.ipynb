{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5023f39d-fd42-40f7-838e-148d340972af",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Finetune Mask R-CNN with MMDetection\n",
    "\n",
    "This notebook uses the following versions:\n",
    "\n",
    "- `mmcv-full==1.5.0`\n",
    "- `mmdet==2.24.1`\n",
    "- `pycocotools==2.0.4`\n",
    "- `torch==1.11.0`\n",
    "- `torchvision==0.12.0+cu113`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00371771-cfa5-4ec8-8f70-413bd56eb520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'mmdetection'...\n",
      "remote: Enumerating objects: 24460, done.\u001b[K\n",
      "remote: Counting objects: 100% (22/22), done.\u001b[K\n",
      "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
      "remote: Total 24460 (delta 3), reused 11 (delta 2), pack-reused 24438\u001b[K\n",
      "Receiving objects: 100% (24460/24460), 37.56 MiB | 36.18 MiB/s, done.\n",
      "Resolving deltas: 100% (17113/17113), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/open-mmlab/mmdetection.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95709b7-e48e-4933-b3b3-c03d0478846d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 1. Define the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9178f5c-48a3-417b-862c-275f1bb38d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mmdetection/mmdet/datasets/mineapple.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mmdetection/mmdet/datasets/mineapple.py\n",
    "from pathlib import Path\n",
    "\n",
    "import mmcv\n",
    "import numpy as np\n",
    "import torch\n",
    "from mmdet.core.mask.utils import mask2bbox, encode_mask_results\n",
    "from mmdet.datasets.builder import DATASETS\n",
    "from mmdet.datasets.coco import CocoDataset\n",
    "from mmdet.datasets.pipelines import Compose\n",
    "from PIL import Image\n",
    "\n",
    "@DATASETS.register_module()\n",
    "class MineAppleDataset(CocoDataset):\n",
    "    CLASSES = (\"fruit\",)\n",
    "    PALETTE = [(220, 20, 60)]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_root,\n",
    "        pipeline,\n",
    "        classes=None,\n",
    "        test_mode=False,\n",
    "        filter_empty_gt=True,\n",
    "        file_client_args=dict(backend='disk'),\n",
    "    ):\n",
    "        self.data_root = Path(data_root)\n",
    "        data_dir = self.data_root / \"detection\"\n",
    "        location = \"test\" if test_mode else \"train\"\n",
    "        split_dir = data_dir / location\n",
    "        self.img_prefix = split_dir / \"images\"\n",
    "        self.seg_prefix = split_dir / \"masks\"\n",
    "        self.test_mode = test_mode\n",
    "        self.filter_empty_gt = filter_empty_gt\n",
    "        self.file_client = mmcv.FileClient(**file_client_args)\n",
    "        self.CLASSES = self.get_classes(classes)\n",
    "\n",
    "        # TODO: Not sure what these are or if needed\n",
    "        self.proposal_file = None\n",
    "        self.proposals = None\n",
    "\n",
    "        # Find all images and corresponding masks\n",
    "        image_paths = list(sorted(self.img_prefix.glob(\"*.png\")))\n",
    "        if not image_paths:\n",
    "            raise RuntimeError(\"No images\")\n",
    "        self.data_infos = []\n",
    "        for image_id, image_path in enumerate(image_paths):\n",
    "            width, height = Image.open(image_path).size\n",
    "            data_info = {\n",
    "                # TODO: id needed?\n",
    "                # \"id\": image_id,\n",
    "                \"width\": width,\n",
    "                \"height\": height,\n",
    "                \"filename\": image_path.name,\n",
    "            }\n",
    "            self.data_infos.append(data_info)\n",
    "\n",
    "        # filter images too small and containing no annotations\n",
    "        if not test_mode:\n",
    "            # valid_inds = self._filter_imgs()\n",
    "            # self.data_infos = [self.data_infos[i] for i in valid_inds]\n",
    "            # set group flag for the sampler\n",
    "            self._set_group_flag()\n",
    "\n",
    "        # processing pipeline\n",
    "        self.pipeline = Compose(pipeline)\n",
    "\n",
    "    def get_ann_info(self, index):\n",
    "        seg_map = self.data_infos[index][\"filename\"]\n",
    "        int_mask = np.array(Image.open(self.seg_prefix / seg_map))\n",
    "\n",
    "        # Convert mask from a 2D image array with objects represented by increasing\n",
    "        # integers to a 3D boolean array\n",
    "        # Source: https://pytorch.org/vision/stable/auto_examples/plot_repurposing_annotations.html\n",
    "        object_ids = np.unique(int_mask)\n",
    "        # Ignore the background object (object_id == 0)\n",
    "        object_ids = object_ids[1:]\n",
    "        masks = int_mask == object_ids[:, None, None]\n",
    "\n",
    "        # Masks to boxes\n",
    "        masks_good = []\n",
    "        boxes = []\n",
    "        for index, mask in enumerate(masks):\n",
    "            y, x = np.where(mask != 0)\n",
    "            box = (np.min(x), np.min(y), np.max(x), np.max(y))\n",
    "            if box[2] <= box[0] or box[3] <= box[1]:\n",
    "                continue\n",
    "            masks_good.append(mask)\n",
    "            boxes.append(box)\n",
    "\n",
    "        # Create the object detection target in its expected format\n",
    "        if boxes:\n",
    "            boxes = np.array(boxes, dtype=np.float32)\n",
    "        else:\n",
    "            boxes = np.zeros((0, 4), dtype=np.float32)\n",
    "        if masks_good:\n",
    "            masks = np.array(masks_good, dtype=np.uint8)\n",
    "        else:\n",
    "            masks = np.zeros((0, *masks.shape[1:]), dtype=np.uint8)\n",
    "        # Assume single label\n",
    "        labels = np.zeros((masks.shape[0],), dtype=np.int64)\n",
    "        # Assume iscrowd is always false\n",
    "        bboxes_ignore = np.zeros((0, 4), dtype=np.float32)\n",
    "        assert boxes.shape[0] == masks.shape[0] == labels.shape[0], (boxes.shape, masks.shape, labels.shape)\n",
    "        assert labels.ndim == 1, labels.ndim\n",
    "        assert bboxes_ignore.shape[0] == 0, bboxes_ignore.shape\n",
    "        assert boxes.shape[1] == bboxes_ignore.shape[1] == 4, (boxes.shape, bboxes_ignore.shape)\n",
    "        assert masks.shape[1:] == (1280, 720), masks.shape\n",
    "        ann = {\n",
    "            \"bboxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"masks\": masks,\n",
    "            \"bboxes_ignore\": bboxes_ignore,\n",
    "            \"seg_map\": seg_map,\n",
    "        }\n",
    "        return ann\n",
    "\n",
    "    # def _filter_imgs(self, **kwargs):\n",
    "    #     # Assume all images are valid\n",
    "    #     self.img_ids = list(range(len(self.data_infos)))\n",
    "    #     return self.img_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "973cb156-e857-49b8-ad1b-c4a75b72d202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mmdetection/mmdet/datasets/pipelines/mineapple.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mmdetection/mmdet/datasets/pipelines/mineapple.py\n",
    "from mmdet.core.mask.structures import BitmapMasks\n",
    "from mmdet.datasets.builder import PIPELINES\n",
    "from mmdet.datasets.pipelines.loading import LoadAnnotations\n",
    "\n",
    "\n",
    "@PIPELINES.register_module()\n",
    "class LoadMineappleAnnotations(LoadAnnotations):\n",
    "    def _load_masks(self, results):\n",
    "        h, w = results['img_info']['height'], results['img_info']['width']\n",
    "        gt_masks = results['ann_info']['masks']\n",
    "        # TODO: Maybe I can disable with_masks and do this in the dataset?...\n",
    "        gt_masks = BitmapMasks(gt_masks, h, w)\n",
    "        results['gt_masks'] = gt_masks\n",
    "        results['mask_fields'].append('gt_masks')\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23a725f-7ea7-4a1d-b1aa-babf3c9fff4b",
   "metadata": {},
   "source": [
    "## Step 2. Fine-tune Mask R-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2577c3-c80b-4d23-8258-16071a531635",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setting PYTHONPATH to make custom_imports in the config work.\n",
    "# Not sure if there's a better way.\n",
    "!PYTHONPATH=mmdetection python mmdetection/tools/train.py fruit_detection.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa37633f-dd4e-42be-b342-ffd4b2c3d310",
   "metadata": {},
   "source": [
    "## Make COCO dataset offline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "feee74e3-5b2d-406f-b275-abda524e6c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from mmdet.core.mask.structures import bitmap_to_polygon\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c0a880f9-7fe4-4207-bd1c-2eca82cf42d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"data/\"\n",
    "test_mode = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "efe9af8e-1d2a-4851-9baf-d57f6d8db851",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [115]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     44\u001b[0m polygons, with_hole \u001b[38;5;241m=\u001b[39m bitmap_to_polygon(mask)\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m with_hole\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(polygons) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     47\u001b[0m polygon \u001b[38;5;241m=\u001b[39m polygons[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_root = Path(data_root)\n",
    "\n",
    "data_dir = data_root / \"detection\"\n",
    "location = \"test\" if test_mode else \"train\"\n",
    "split_dir = data_dir / location\n",
    "img_prefix = split_dir / \"images\"\n",
    "seg_prefix = split_dir / \"masks\"\n",
    "\n",
    "# Find all images and corresponding masks\n",
    "image_paths = list(sorted(img_prefix.glob(\"*.png\")))\n",
    "if not image_paths:\n",
    "    raise RuntimeError(\"No images\")\n",
    "\n",
    "DEFAULT_CATEGORY_ID = 1\n",
    "categories = [{\"id\": DEFAULT_CATEGORY_ID, \"name\": \"fruit\"}]\n",
    "images, annotations = [], []\n",
    "annotation_id = 1\n",
    "for image_id, image_path in enumerate(image_paths):\n",
    "    width, height = Image.open(image_path).size\n",
    "    image = {\n",
    "        \"id\": image_id,\n",
    "        \"width\": width,\n",
    "        \"height\": height,\n",
    "        \"file_name\": image_path.name,\n",
    "        # TODO\n",
    "        \"coco_url\": None,\n",
    "    }\n",
    "    images.append(image)\n",
    "\n",
    "    int_mask = np.array(Image.open(seg_prefix / image_path.name))\n",
    "    # Convert mask from a 2D image array with objects represented by increasing\n",
    "    # integers to a 3D boolean array\n",
    "    # Source: https://pytorch.org/vision/stable/auto_examples/plot_repurposing_annotations.html\n",
    "    object_ids = np.unique(int_mask)\n",
    "    # Ignore the background object (object_id == 0)\n",
    "    object_ids = object_ids[1:]\n",
    "    masks = int_mask == object_ids[:, None, None]\n",
    "    # Create individual annotations\n",
    "    for index, mask in enumerate(masks):\n",
    "        y, x = np.where(mask != 0)\n",
    "        bbox = (np.min(x), np.min(y), np.max(x), np.max(y))\n",
    "        if bbox[2] <= bbox[0] or bbox[3] <= bbox[1]:\n",
    "            continue\n",
    "        polygons, with_hole = bitmap_to_polygon(mask)\n",
    "        assert not with_hole\n",
    "        assert len(polygons) == 1\n",
    "        polygon = polygons[0]\n",
    "        segmentation = [polygon.ravel().tolist()]\n",
    "        area = mask.sum()\n",
    "        annotation = {\n",
    "            \"id\": annotation_id,\n",
    "            \"image_id\": image_id,\n",
    "            \"segmentation\": segmentation,\n",
    "            \"category_id\": DEFAULT_CATEGORY_ID,\n",
    "            \"iscrowd\": 0,\n",
    "            \"bbox\": bbox,\n",
    "            \"area\": area,\n",
    "        }\n",
    "        annotations.append(annotation)\n",
    "        annotation_id += 1\n",
    "        \n",
    "dataset = {\"images\": images, \"annotations\": annotations, \"categories\": categories}\n",
    "coco = COCO()\n",
    "coco.dataset = dataset\n",
    "coco.createIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4fe0c1-cc17-4b66-b33c-b1180687b4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"mmdetection/\n",
    "coco = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f0719711-b6e9-48c1-b150-a33dcb41395d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "cf70a52f-c6f3-4c5b-b6f5-650300afb9fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f0d691d5a30>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "3734114a-8250-4c2b-9481-42a17d8c849a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 63, 705],\n",
       "        [ 62, 706],\n",
       "        [ 61, 706],\n",
       "        [ 60, 706],\n",
       "        [ 59, 706],\n",
       "        [ 58, 706],\n",
       "        [ 57, 707],\n",
       "        [ 56, 708],\n",
       "        [ 55, 709],\n",
       "        [ 54, 710],\n",
       "        [ 54, 711],\n",
       "        [ 53, 712],\n",
       "        [ 52, 713],\n",
       "        [ 51, 714],\n",
       "        [ 50, 715],\n",
       "        [ 50, 716],\n",
       "        [ 50, 717],\n",
       "        [ 50, 718],\n",
       "        [ 50, 719],\n",
       "        [ 50, 720],\n",
       "        [ 49, 721],\n",
       "        [ 49, 722],\n",
       "        [ 49, 723],\n",
       "        [ 49, 724],\n",
       "        [ 49, 725],\n",
       "        [ 49, 726],\n",
       "        [ 50, 727],\n",
       "        [ 50, 728],\n",
       "        [ 51, 729],\n",
       "        [ 51, 730],\n",
       "        [ 52, 731],\n",
       "        [ 52, 732],\n",
       "        [ 53, 733],\n",
       "        [ 53, 734],\n",
       "        [ 54, 735],\n",
       "        [ 55, 735],\n",
       "        [ 56, 736],\n",
       "        [ 57, 736],\n",
       "        [ 58, 737],\n",
       "        [ 59, 737],\n",
       "        [ 60, 738],\n",
       "        [ 61, 738],\n",
       "        [ 62, 739],\n",
       "        [ 63, 739],\n",
       "        [ 64, 739],\n",
       "        [ 65, 738],\n",
       "        [ 66, 738],\n",
       "        [ 67, 738],\n",
       "        [ 68, 738],\n",
       "        [ 69, 738],\n",
       "        [ 70, 738],\n",
       "        [ 71, 737],\n",
       "        [ 72, 737],\n",
       "        [ 73, 737],\n",
       "        [ 74, 736],\n",
       "        [ 75, 735],\n",
       "        [ 75, 734],\n",
       "        [ 76, 733],\n",
       "        [ 77, 732],\n",
       "        [ 78, 731],\n",
       "        [ 79, 730],\n",
       "        [ 80, 729],\n",
       "        [ 80, 728],\n",
       "        [ 81, 727],\n",
       "        [ 82, 726],\n",
       "        [ 81, 726],\n",
       "        [ 80, 727],\n",
       "        [ 79, 727],\n",
       "        [ 78, 727],\n",
       "        [ 77, 727],\n",
       "        [ 76, 728],\n",
       "        [ 75, 728],\n",
       "        [ 74, 728],\n",
       "        [ 73, 728],\n",
       "        [ 72, 727],\n",
       "        [ 73, 726],\n",
       "        [ 74, 725],\n",
       "        [ 75, 724],\n",
       "        [ 76, 723],\n",
       "        [ 76, 722],\n",
       "        [ 77, 721],\n",
       "        [ 78, 720],\n",
       "        [ 79, 719],\n",
       "        [ 80, 718],\n",
       "        [ 81, 717],\n",
       "        [ 80, 717],\n",
       "        [ 79, 716],\n",
       "        [ 78, 716],\n",
       "        [ 77, 716],\n",
       "        [ 76, 716],\n",
       "        [ 75, 717],\n",
       "        [ 74, 717],\n",
       "        [ 73, 718],\n",
       "        [ 72, 718],\n",
       "        [ 71, 718],\n",
       "        [ 70, 719],\n",
       "        [ 69, 719],\n",
       "        [ 68, 720],\n",
       "        [ 67, 720],\n",
       "        [ 66, 720],\n",
       "        [ 65, 721],\n",
       "        [ 64, 721],\n",
       "        [ 63, 721],\n",
       "        [ 62, 722],\n",
       "        [ 61, 722],\n",
       "        [ 60, 723],\n",
       "        [ 59, 723],\n",
       "        [ 58, 723],\n",
       "        [ 57, 724],\n",
       "        [ 56, 724],\n",
       "        [ 55, 725],\n",
       "        [ 54, 725],\n",
       "        [ 53, 725],\n",
       "        [ 52, 724],\n",
       "        [ 53, 723],\n",
       "        [ 53, 722],\n",
       "        [ 53, 721],\n",
       "        [ 54, 720],\n",
       "        [ 54, 719],\n",
       "        [ 55, 718],\n",
       "        [ 55, 717],\n",
       "        [ 55, 716],\n",
       "        [ 56, 715],\n",
       "        [ 57, 714],\n",
       "        [ 58, 713],\n",
       "        [ 59, 713],\n",
       "        [ 60, 712],\n",
       "        [ 61, 712],\n",
       "        [ 62, 711],\n",
       "        [ 63, 711],\n",
       "        [ 64, 710],\n",
       "        [ 65, 710],\n",
       "        [ 66, 710],\n",
       "        [ 67, 710],\n",
       "        [ 68, 710],\n",
       "        [ 68, 709],\n",
       "        [ 67, 708],\n",
       "        [ 67, 707],\n",
       "        [ 66, 706],\n",
       "        [ 66, 705],\n",
       "        [ 65, 705],\n",
       "        [ 64, 705]], dtype=int32),\n",
       " array([[ 71, 728],\n",
       "        [ 72, 727],\n",
       "        [ 73, 728],\n",
       "        [ 72, 729]], dtype=int32)]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ecb390ee-05c1-4d9f-87eb-4609557dd9c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([[149, 144],\n",
       "         [148, 145],\n",
       "         [147, 145],\n",
       "         [146, 145],\n",
       "         [145, 146],\n",
       "         [144, 146],\n",
       "         [143, 146],\n",
       "         [142, 146],\n",
       "         [141, 147],\n",
       "         [140, 147],\n",
       "         [140, 148],\n",
       "         [139, 149],\n",
       "         [139, 150],\n",
       "         [139, 151],\n",
       "         [139, 152],\n",
       "         [138, 153],\n",
       "         [138, 154],\n",
       "         [138, 155],\n",
       "         [138, 156],\n",
       "         [137, 157],\n",
       "         [137, 158],\n",
       "         [137, 159],\n",
       "         [137, 160],\n",
       "         [138, 161],\n",
       "         [138, 162],\n",
       "         [139, 163],\n",
       "         [139, 164],\n",
       "         [139, 165],\n",
       "         [140, 166],\n",
       "         [140, 167],\n",
       "         [141, 168],\n",
       "         [141, 169],\n",
       "         [142, 169],\n",
       "         [143, 169],\n",
       "         [144, 169],\n",
       "         [145, 169],\n",
       "         [146, 169],\n",
       "         [147, 169],\n",
       "         [148, 168],\n",
       "         [149, 168],\n",
       "         [150, 168],\n",
       "         [151, 168],\n",
       "         [152, 168],\n",
       "         [153, 168],\n",
       "         [154, 168],\n",
       "         [155, 168],\n",
       "         [156, 168],\n",
       "         [156, 167],\n",
       "         [157, 166],\n",
       "         [157, 165],\n",
       "         [158, 164],\n",
       "         [158, 163],\n",
       "         [159, 162],\n",
       "         [159, 161],\n",
       "         [160, 160],\n",
       "         [160, 159],\n",
       "         [159, 158],\n",
       "         [159, 157],\n",
       "         [158, 156],\n",
       "         [157, 155],\n",
       "         [157, 154],\n",
       "         [156, 153],\n",
       "         [155, 152],\n",
       "         [155, 151],\n",
       "         [154, 150],\n",
       "         [153, 149],\n",
       "         [153, 148],\n",
       "         [152, 147],\n",
       "         [151, 146],\n",
       "         [151, 145],\n",
       "         [150, 144]], dtype=int32)],\n",
       " False)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fedf8a9-8aa6-4b5d-a982-9499bae8e337",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "from mmdet.utils import get_device\n",
    "from pycocotools import mask as coco_mask\n",
    "\n",
    "# annotation IDs need to start at 1, not 0, see torchvision issue #1530\n",
    "ann_id = 1\n",
    "dataset = {\"images\": [], \"categories\": [], \"annotations\": []}\n",
    "categories = set()\n",
    "mb = master_bar(range(len(ds)))\n",
    "for img_idx in mb:\n",
    "    mb.main_bar.comment = \"images\"\n",
    "\n",
    "    data_info = ds.data_infos[img_idx]\n",
    "    img_dict = {}\n",
    "    img_dict[\"id\"] = img_idx\n",
    "    img_dict[\"height\"] = data_info[\"height\"]\n",
    "    img_dict[\"width\"] = data_info[\"height\"]\n",
    "    dataset[\"images\"].append(img_dict)\n",
    "\n",
    "    ann_info = ds.get_ann_info(img_idx)\n",
    "    labels = ann_info[\"labels\"].tolist()\n",
    "    bboxes = ann_info[\"bboxes\"].copy()\n",
    "    bboxes[:, 2:] -= bboxes[:, :2]\n",
    "    areas = bboxes[:, 2:].prod(axis=-1)\n",
    "    bboxes = bboxes.tolist()\n",
    "    num_objs = len(bboxes)\n",
    "    iscrowd = [0] * num_objs\n",
    "    if \"masks\" in ann_info:\n",
    "        masks = ann_info[\"masks\"]\n",
    "    for i in progress_bar(range(num_objs), parent=mb):\n",
    "        mb.child.comment = \"annotations\"\n",
    "        ann = {}\n",
    "        ann[\"image_id\"] = img_idx\n",
    "        ann[\"bbox\"] = bboxes[i]\n",
    "        ann[\"category_id\"] = labels[i]\n",
    "        categories.add(labels[i])\n",
    "        ann[\"area\"] = areas[i]\n",
    "        ann[\"iscrowd\"] = iscrowd[i]\n",
    "        ann[\"id\"] = ann_id\n",
    "        if \"masks\" in ann_info:\n",
    "            # NOTE: I'm not sure what this does exactly but it's needed by pycocotools.\n",
    "            # TODO: Is there a way to avoid doing this on every slice?\n",
    "            ann[\"segmentation\"] = coco_mask.encode(np.asfortranarray(masks[i]))\n",
    "        dataset[\"annotations\"].append(ann)\n",
    "        ann_id += 1\n",
    "dataset[\"categories\"] = [{\"id\": i} for i in sorted(categories)]\n",
    "\n",
    "coco = COCO()\n",
    "coco.dataset = dataset\n",
    "coco.createIndex()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e340d63-0721-4e47-8176-cf473ec1c637",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Make COCO API from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ddf9f73-4941-493b-a395-4236ca03ac3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"mmdetection\" not in sys.path:\n",
    "    sys.path.insert(0, \"mmdetection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de40c75e-6881-4c92-aced-a29b62d81ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmcv import Config\n",
    "from mmdet.datasets import build_dataset, build_dataloader\n",
    "from mmdet.datasets.api_wrappers.coco_api import COCO\n",
    "from mmdet.models import build_detector\n",
    "\n",
    "config = \"fruit_detection.py\"\n",
    "cfg = Config.fromfile(config)\n",
    "ds = build_dataset(cfg.data.train)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "from mmdet.utils import get_device\n",
    "from pycocotools import mask as coco_mask\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "# annotation IDs need to start at 1, not 0, see torchvision issue #1530\n",
    "ann_id = 1\n",
    "dataset = {\"images\": [], \"categories\": [], \"annotations\": []}\n",
    "categories = set()\n",
    "mb = master_bar(range(len(ds)))\n",
    "for img_idx in mb:\n",
    "    mb.main_bar.comment = \"images\"\n",
    "\n",
    "    data_info = ds.data_infos[img_idx]\n",
    "    img_dict = {}\n",
    "    img_dict[\"id\"] = img_idx\n",
    "    img_dict[\"height\"] = data_info[\"height\"]\n",
    "    img_dict[\"width\"] = data_info[\"height\"]\n",
    "    dataset[\"images\"].append(img_dict)\n",
    "\n",
    "    ann_info = ds.get_ann_info(img_idx)\n",
    "    labels = ann_info[\"labels\"].tolist()\n",
    "    bboxes = ann_info[\"bboxes\"].copy()\n",
    "    bboxes[:, 2:] -= bboxes[:, :2]\n",
    "    areas = bboxes[:, 2:].prod(axis=-1)\n",
    "    bboxes = bboxes.tolist()\n",
    "    num_objs = len(bboxes)\n",
    "    iscrowd = [0] * num_objs\n",
    "    if \"masks\" in ann_info:\n",
    "        masks = ann_info[\"masks\"]\n",
    "    for i in progress_bar(range(num_objs), parent=mb):\n",
    "        mb.child.comment = \"annotations\"\n",
    "        ann = {}\n",
    "        ann[\"image_id\"] = img_idx\n",
    "        ann[\"bbox\"] = bboxes[i]\n",
    "        ann[\"category_id\"] = labels[i]\n",
    "        categories.add(labels[i])\n",
    "        ann[\"area\"] = areas[i]\n",
    "        ann[\"iscrowd\"] = iscrowd[i]\n",
    "        ann[\"id\"] = ann_id\n",
    "        if \"masks\" in ann_info:\n",
    "            # NOTE: I'm not sure what this does exactly but it's needed by pycocotools.\n",
    "            # TODO: Is there a way to avoid doing this on every slice?\n",
    "            ann[\"segmentation\"] = coco_mask.encode(np.asfortranarray(masks[i]))\n",
    "        dataset[\"annotations\"].append(ann)\n",
    "        ann_id += 1\n",
    "dataset[\"categories\"] = [{\"id\": i} for i in sorted(categories)]\n",
    "\n",
    "coco = COCO()\n",
    "coco.dataset = dataset\n",
    "coco.createIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ca1baec1-a507-4da9-97c0-0113ad8f77c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 0, 'height': 1280, 'width': 1280}]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coco.loadImgs(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cf404d19-5eae-4a64-807f-580f4cea98a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'image_id': 0,\n",
       "  'bbox': [137.0, 144.0, 23.0, 25.0],\n",
       "  'category_id': 0,\n",
       "  'area': 575.0,\n",
       "  'iscrowd': 0,\n",
       "  'id': 1,\n",
       "  'segmentation': {'size': [1280, 720],\n",
       "   'counts': b'm\\\\[54hW16J7K4N2M1000000O1000O01001O2N1O2N1O2N1M4L3M4LScje0'}}]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coco.loadAnns(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb8134c-3078-4fe6-820b-a3f18993647c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 3. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661b027c-19e3-47cb-90a9-e5b196c02e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"mmdetection\" not in sys.path:\n",
    "    sys.path.insert(0, \"mmdetection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe21c52-5b0d-4b9f-9c1a-c442cb58ec7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"fruit_detection.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db0ebe4-88a5-470d-8237-7406e9fbae5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmcv import Config\n",
    "from mmdet.datasets import build_dataset, build_dataloader\n",
    "from mmdet.models import build_detector\n",
    "\n",
    "cfg = Config.fromfile(config)\n",
    "dataset = build_dataset(cfg.data.train)\n",
    "model = build_detector(cfg.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ea6feca-37e8-4f9c-804b-12e16e1adcc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "670"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a8d712-60bf-4cc0-a954-ce72530297bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmdet.apis import init_detector, inference_detector\n",
    "\n",
    "model = build_detector(cfg.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04d0e20-dbd0-464b-abb7-dc5ad2f83452",
   "metadata": {},
   "source": [
    "## Rough: Train in code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6a218ffa-e346-4743-9e6f-bce07c070f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmcv import Config\n",
    "from mmdet.datasets import build_dataset, build_dataloader\n",
    "from mmdet.models import build_detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "87303267-c8f0-4f39-a2a5-eec22ea8c126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/jupyter/ai-playground/object-detection',\n",
       " '/opt/conda/lib/python37.zip',\n",
       " '/opt/conda/lib/python3.7',\n",
       " '/opt/conda/lib/python3.7/lib-dynload',\n",
       " '',\n",
       " '/opt/conda/lib/python3.7/site-packages',\n",
       " '/opt/conda/lib/python3.7/site-packages/IPython/extensions',\n",
       " '/home/jupyter/.ipython',\n",
       " 'mmdetection']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "if \"mmdetection\" not in sys.path:\n",
    "    sys.path.append(\"mmdetection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e2ab08ff-275c-4c01-a475-8c8bfc972bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"fruit_detection.py\"\n",
    "\n",
    "cfg = Config.fromfile(config)\n",
    "\n",
    "args = cfg.data.train.copy()\n",
    "args.pop(\"type\")\n",
    "dataset = MineAppleDataset(**args)\n",
    "# datasets = [build_dataset(cfg.data.train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "049bec5c-1007-4fc1-bc32-8e14dc5a6255",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_detector(cfg.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2c2c842-d879-4076-bdfa-90f011a9d011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single GPU training\n",
    "cfg.gpu_ids = range(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e2c0671-c504-49d9-8efd-9a43935e41b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributed as dist\n",
    "from mmdet.apis import init_random_seed, set_random_seed, train_detector\n",
    "from mmdet.utils import (collect_env, get_device, get_root_logger,\n",
    "                         setup_multi_processes, update_data_root)\n",
    "\n",
    "deterministic = False\n",
    "seed = 0\n",
    "\n",
    "cfg.device = get_device()\n",
    "\n",
    "# set random seeds\n",
    "seed = init_random_seed(seed, device=cfg.device)\n",
    "set_random_seed(seed, deterministic=deterministic)\n",
    "cfg.seed = seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bf5acb9-8e21-4f96-b517-084879737ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "distributed = False\n",
    "runner_type = 'EpochBasedRunner' if 'runner' not in cfg else cfg.runner['type']\n",
    "train_dataloader_default_args = dict(\n",
    "    samples_per_gpu=2,\n",
    "    workers_per_gpu=2,\n",
    "    # `num_gpus` will be ignored if distributed\n",
    "    num_gpus=len(cfg[\"gpu_ids\"]),\n",
    "    dist=distributed,\n",
    "    seed=cfg[\"seed\"],\n",
    "    runner_type=runner_type,\n",
    "    persistent_workers=False)\n",
    "\n",
    "train_loader_cfg = {\n",
    "    **train_dataloader_default_args,\n",
    "    **cfg.data.get('train_dataloader', {})\n",
    "}\n",
    "data_loader = build_dataloader(dataset, **train_loader_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5215f7a3-911b-4526-89b7-bf01f96e1cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from mmdet.apis.train import train_detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d3ceb65-89b4-40c7-8271-a9a70eead709",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_validate = False\n",
    "\n",
    "timestamp = time.strftime('%Y%m%d_%H%M%S', time.localtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "272b290a-c853-4e3d-80e2-60396e0013e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.work_dir = os.path.join('./work_dirs',\n",
    "                        os.path.splitext(os.path.basename(config))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686a145a-cb7e-45f4-ba69-af03152dc5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info_dict = collect_env()\n",
    "env_info = '\\n'.join([(f'{k}: {v}') for k, v in env_info_dict.items()])\n",
    "meta = {\n",
    "    \"env_info\": env_info,\n",
    "    \"config\": cfg.pretty_text,\n",
    "    \"seed\": cfg.seed,\n",
    "    \"exp_name\": os.path.basename(config)\n",
    "}\n",
    "\n",
    "train_detector(model, [dataset], cfg, distributed=distributed, validate=(not no_validate), timestamp=timestamp, meta=meta)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-11.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-11:m91"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
